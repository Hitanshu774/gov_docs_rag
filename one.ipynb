{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17f9419f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: module 'importlib.metadata' has no attribute 'packages_distributions'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hitan\\anaconda3\\envs\\torch_llm\\lib\\site-packages\\google\\api_core\\_python_version_support.py:237: FutureWarning: You are using a non-supported Python version (3.9.25). Google will not post any further updates to google.api_core supporting this Python version. Please upgrade to the latest Python version, or at least Python 3.10, and then update google.api_core.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "c:\\Users\\hitan\\anaconda3\\envs\\torch_llm\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0efe1bf",
   "metadata": {},
   "source": [
    "# Using langchain loaders to load dataset from local directory -> corpus.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d148e4f5",
   "metadata": {},
   "source": [
    "üëâ Each JSONL line is being passed as a full Python dict\n",
    "üëâ LangChain expects Document.page_content to be a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d71e47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 973 documents\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path=\"corpus.jsonl\",\n",
    "    # jq_schema=\"._id\",\n",
    "    jq_schema=\".text\",\n",
    "    json_lines=True\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "print(f\"Loaded {len(documents)} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "640fe7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"Privileged\" Nominations Every year the Senate routinely considers whether to give its advice and consent to hundreds of nominations submitted by the President. From start to finish, the confirmation process can be a lengthy one, even for relatively noncontroversial nominees. Each nomination is typically referred to one or more committees having subject matter jurisdiction over the position. Committees may bear a significant workload in examining nominees√¢\\x80\\x94often including questionnaires, option'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(documents[0]))\n",
    "documents[0].page_content[:500]  # Print the first 500 characters of the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89dac5ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'C:\\\\Users\\\\hitan\\\\OneDrive\\\\Desktop\\\\me space\\\\Projects_RAG\\\\gov_docs_rag\\\\gov_docs_rag\\\\corpus.jsonl',\n",
       " 'seq_num': 5}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[4].metadata  # Print the metadata of the first document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ce90839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 61375 chunks\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "textsplitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "chunks = textsplitter.split_documents(documents)\n",
    "print(f\"Split into {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e654c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Privileged\" Nominations Every year the Senate routinely considers whether to give its advice and consent to hundreds of nominations submitted by the President. From start to finish, the confirmation process can be a lengthy one, even for relatively noncontroversial nominees. Each nomination is typically referred to one or more committees having subject matter jurisdiction over the position. Committees may bear a significant workload in examining nominees√¢\\x80\\x94often including questionnaires, optional public hearings, and individual meetings with Senators√¢\\x80\\x94to determine whether to report a nomination to the full Senate. Once a committee has reported a nomination or been discharged from its further consideration, the Senate may take up a nomination for deliberation, though a cloture process may be required to ensure a final vote to confirm. As part of an effort to streamline the nominations process during the 112 th Congress (2011-2012), a standing order of the Senate, S.Res. 116 , created'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0].page_content # Print the first chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15c232ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = HuggingFaceEmbeddings(model_name = \"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "# db_name = \"vector_db\"\n",
    "\n",
    "# if os.path.exists(db_name):\n",
    "#     Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "    \n",
    "# vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "# print(vectorstore._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b5618a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## relaoding the vectordb without re-embedding\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\"\n",
    ")\n",
    "\n",
    "vectordb = Chroma(\n",
    "    persist_directory=\"./vector_db\",\n",
    "    embedding_function=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a32c4ce",
   "metadata": {},
   "source": [
    "setting up langchain objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92a961d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEY: 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\n",
      "Starts with sk-or-: True\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "key = os.getenv(\"api-key\")\n",
    "print(\"KEY:\", key)\n",
    "print(\"Starts with sk-or-:\", key.startswith(\"sk-or-\") if key else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7904d85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":5})\n",
    "llm = ChatOpenAI(model_name=\"mistralai/devstral-2512:free\", \n",
    "                 openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "                 temperature=0.3,\n",
    "                 max_tokens=512,\n",
    "                 openai_api_key=os.getenv(\"api-key\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2092ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='9ee824a8-0899-4f42-b359-989b22099a94', metadata={'seq_num': 274, 'source': 'C:\\\\Users\\\\hitan\\\\OneDrive\\\\Desktop\\\\me space\\\\Projects_RAG\\\\gov_docs_rag\\\\gov_docs_rag\\\\corpus.jsonl'}, page_content='Enterprise Acquisition Services. Services Provided: Computing Services operates the DISA Data Centers, which provide mainframe and server processing operations, data storage, and other information technology services and support across the Department of Defense (DOD). Telecommunications Services provides secure telecommunications services, including the Defense Information Systems Network. Enterprise Acquisitions Services provides contracting services for information technology and telecommunications acquisitions from the commercial sector and contracting support to the Defense Information Systems Network programs and other customers through DISA‚Äôs Defense Information Technology Contracting Organization. Approach to Allocating Costs: The Defense Information Systems Agency (DISA) groups its services by the costs associated with providing them. These costs are specific to the service being provided and are influenced by factors such as the cost of equipment used to provide the service.'),\n",
       " Document(id='87bfa4e6-b50a-4070-83cc-905c5c107622', metadata={'seq_num': 879, 'source': 'C:\\\\Users\\\\hitan\\\\OneDrive\\\\Desktop\\\\me space\\\\Projects_RAG\\\\gov_docs_rag\\\\gov_docs_rag\\\\corpus.jsonl'}, page_content='contracts achieved desired outcomes. While DOD agreed with the recommendation and developed a template for the military departments to use to collect relevant information, it is still gathering updates from the military departments about the status of this effort. Contracting for Major Defense Acquisition Programs DOD acquires MDAPs through the Defense Acquisition System, which implements an adaptive acquisition framework that allows DOD officials to develop acquisition strategies and employ acquisition processes that match the characteristics of the capability being acquired. The pathway for acquiring major capabilities generally includes four phases, three of which we focus on in this report: (1) technology maturation and risk reduction; (2) engineering and manufacturing development; and (3) production and deployment. Programs typically complete a series of milestone reviews and other key decision points that authorize entry into a new acquisition phase, as illustrated in figure 2.'),\n",
       " Document(id='42bec451-626c-46b8-929f-c5affcbf58a4', metadata={'seq_num': 4, 'source': 'C:\\\\Users\\\\hitan\\\\OneDrive\\\\Desktop\\\\me space\\\\Projects_RAG\\\\gov_docs_rag\\\\gov_docs_rag\\\\corpus.jsonl'}, page_content='Background DOD acquires new weapon systems for its warfighters through a management process known as the Defense Acquisition System. This system is implemented by two key acquisition policies: DOD Directive 5000.01, which establishes the overarching framework for the Defense Acquisition System; and DOD Instruction 5000.02, which provides detailed procedures for the operation of the Defense Acquisition System and the management of acquisition programs. These policy documents establish the guiding principles for all aspects of the DOD acquisition process. Additionally, each of the military services has its own acquisition policies which incorporate and enhance the DOD acquisition guidance. Figure 2 depicts DOD‚Äôs acquisition process beginning with Milestone A in general terms. Several entities in the Office of the Secretary of Defense and the military departments play a role in the oversight of DOD weapon system acquisition programs, including the following: The Under Secretary of Defense'),\n",
       " Document(id='0c9a07d2-2660-4917-98f5-e0e7a2d1f418', metadata={'source': 'C:\\\\Users\\\\hitan\\\\OneDrive\\\\Desktop\\\\me space\\\\Projects_RAG\\\\gov_docs_rag\\\\gov_docs_rag\\\\corpus.jsonl', 'seq_num': 13}, page_content='required in traditional mechanisms, such as contracts or grants. Congress also required that DOD establish a panel in the National Defense Authorization Act for Fiscal Year 2016, referred to as the ‚ÄúSection 809 Panel,‚Äù to identify ways to streamline and improve the defense acquisition system. The panel issued its final report in January 2019, which, together with its earlier reports, included a wide range of recommendations aimed at changing the overall structure and operations of defense acquisition. DOD Acquisition Programs and Authorities DOD acquisition policy defines an acquisition program as a directed, funded effort that provides a new, improved, or continuing materiel, weapon, or information system, or a service capability in response to an approved need. DOD Directive 5000.01, The Defense Acquisition System, provides management principles and mandatory policies and procedures for managing all acquisition programs. Oversight levels and procedures for DOD‚Äôs acquisition programs'),\n",
       " Document(id='4acc2bbb-6c51-488e-9a84-6ed4a261dadb', metadata={'source': 'C:\\\\Users\\\\hitan\\\\OneDrive\\\\Desktop\\\\me space\\\\Projects_RAG\\\\gov_docs_rag\\\\gov_docs_rag\\\\corpus.jsonl', 'seq_num': 13}, page_content='because technologies were considered mature by the Office of the Secretary of Defense and an independent review team, respectively. Figure 1 illustrates the key milestones associated with the defense acquisition system. DOD‚Äôs acquisition policy encourages tailoring the acquisition process, including tailoring of documentation or information requirements. In previous work, we identified opportunities for DOD to tailor the documentation and oversight needed for major defense acquisition programs. In 2015, we found that 24 acquisition programs we surveyed spent, on average, over 2 years completing up to 49 information requirements for their most recent milestone decision. We found that DOD‚Äôs review process was a key factor that influenced the time needed to complete the information requirements. In total, the requirements averaged 5,600 staff days to document, yet acquisition officials considered only about half of the requirements as high value. We recommended that DOD eliminate reviews')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What is Defense Acquisition System?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cecbeb96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The **Defense Acquisition System (DAS)** is the structured process used by the **U.S. Department of Defense (DoD)** to acquire weapons, equipment, services, and other capabilities needed to support national defense. It is governed by **DoD Directive 5000.01** and **DoD Instruction 5000.02**, which outline policies and procedures for acquiring defense systems efficiently, cost-effectively, and in a timely manner.\\n\\n### **Key Components of the Defense Acquisition System:**\\n1. **Acquisition Framework**\\n   - The system follows a **phased approach** (from concept to disposal) to manage risk, cost, and performance.\\n   - It emphasizes **affordability, innovation, and rapid fielding** of capabilities.\\n\\n2. **Acquisition Phases (DoD 5000.02)**\\n   The process is divided into **six key phases**:\\n   - **Materiel Solution Analysis (MSA)** ‚Äì Identifies capabilities needed and explores potential solutions.\\n   - **Technology Maturation & Risk Reduction (TMRR)** ‚Äì Develops and tests technologies to reduce risks.\\n   - **Engineering & Manufacturing Development (EMD)** ‚Äì Finalizes system design and begins low-rate production.\\n   - **Production & Deployment (P&D)** ‚Äì Full-scale manufacturing and fielding of the system.\\n   - **Operations & Support (O&S)** ‚Äì Sustains the system throughout its lifecycle.\\n   - **Disposal** ‚Äì Retires or replaces the system when no longer needed.\\n\\n3. **Key Stakeholders**\\n   - **Acquisition Workforce** (Program Managers, Engineers, Contracting Officers)\\n   - **Military Services** (Army, Navy, Air Force, Space Force, Marines)\\n   - **Defense Agencies** (DARPA, Missile Defense Agency, etc.)\\n   - **Industry Partners** (Defense contractors like Lockheed Martin, Boeing, etc.)\\n   - **Congress** (Oversight and funding approval)\\n\\n4. **Acquisition Pathways**\\n   The DoD uses different **acquisition pathways** based on the complexity and urgency of the program:\\n   - **Major Capability Acquisition (MCA)** ‚Äì For large, complex systems (e.g., F-35, aircraft carriers).\\n   - **Middle-Tier Acquisition (MTA)** ‚Äì For rapid prototyping and fielding (e.g., hypersonic missiles).\\n   - **Urgent Capability', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 512, 'prompt_tokens': 9, 'total_tokens': 521, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_name': 'mistralai/devstral-2512:free', 'system_fingerprint': None, 'id': 'gen-1768441515-exu2JAVH6pH4w0YIjzMI', 'service_tier': None, 'finish_reason': 'length', 'logprobs': None}, id='run--637dff8a-b431-4620-88f5-7fdc47ca55e0-0', usage_metadata={'input_tokens': 9, 'output_tokens': 512, 'total_tokens': 521, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is Defense Acquisition System?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f11fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_TEMPLATE = \"\"\"\n",
    "You are a knowledgeable, strict assistant representing the details from government documents.\n",
    "You are chatting with a user about government policies.\n",
    "If relevant, use the given context to answer any question.\n",
    "If you don't know the answer, say so.\n",
    "Context:\n",
    "{context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742dc0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question: str, history):\n",
    "    docs = retriever.invoke(question)\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in docs)####################\n",
    "    system_prompt = SYSTEM_PROMPT_TEMPLATE.format(context=context)\n",
    "    response = llm.invoke([SystemMessage(content=system_prompt), HumanMessage(content=question)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799a3e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the provided context, none of the mentioned programs had an estimated combined value of $20 billion. The largest funding figure mentioned is the **$1.2 trillion** estimate over 30 years for the **U.S. nuclear arsenal modernization program**, as referenced in the Congressional Budget Office's projection.\\n\\nIf you're referring to a different program or need further clarification, please specify.\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# answer_question(\"which program had an estimated combined value of $20 billion?\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc24cf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gr.ChatInterface(answer_question).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f5029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm1 = ChatOpenAI(\n",
    "    model=\"nvidia/nemotron-3-nano-30b-a3b:free\",  # example\n",
    "    openai_api_key=os.getenv(\"api-key\"),\n",
    "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "    temperature=0.3,\n",
    "    max_tokens=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe046a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create another llm instance with different model and test the performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74530f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'tests.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 289\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m    288\u001b[0m test_cases \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 289\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtests.jsonl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m    291\u001b[0m         test_cases\u001b[38;5;241m.\u001b[39mappend(json\u001b[38;5;241m.\u001b[39mloads(line\u001b[38;5;241m.\u001b[39mstrip()))\n",
      "File \u001b[1;32mc:\\Users\\hitan\\anaconda3\\envs\\torch_llm\\lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tests.jsonl'"
     ]
    }
   ],
   "source": [
    "from rag_evaluator import RAGEvaluator\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "# ============================================================================\n",
    "# DATA CLASSES FOR DETAILED EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class RetrievalResult:\n",
    "    \"\"\"Results from retrieval evaluation\"\"\"\n",
    "    mrr: float  # Mean Reciprocal Rank\n",
    "    ndcg: float  # Normalized Discounted Cumulative Gain\n",
    "    keywords_found: int\n",
    "    total_keywords: int\n",
    "    keyword_coverage: float  # percentage\n",
    "    retrieved_docs: List[str]\n",
    "\n",
    "@dataclass\n",
    "class AnswerResult:\n",
    "    \"\"\"Results from answer evaluation\"\"\"\n",
    "    accuracy: float  # 0-5\n",
    "    completeness: float  # 0-5\n",
    "    relevance: float  # 0-5\n",
    "    feedback: str\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_retrieval(question, keywords, k=4):\n",
    "    \"\"\"\n",
    "    Evaluate retrieval quality\n",
    "    Returns: RetrievalResult with metrics\n",
    "    \"\"\"\n",
    "    docs = retreiver.invoke(question)\n",
    "    \n",
    "    # Get doc content\n",
    "    doc_contents = [doc.page_content.lower() for doc in docs]\n",
    "    combined_content = \" \".join(doc_contents)\n",
    "    \n",
    "    # Count keywords found\n",
    "    keywords_found = 0\n",
    "    for keyword in keywords:\n",
    "        if keyword.lower() in combined_content:\n",
    "            keywords_found += 1\n",
    "    \n",
    "    # Calculate keyword coverage\n",
    "    keyword_coverage = (keywords_found / len(keywords) * 100) if keywords else 0\n",
    "    \n",
    "    # Calculate MRR (Mean Reciprocal Rank)\n",
    "    mrr = 0.0\n",
    "    for idx, doc in enumerate(docs):\n",
    "        doc_lower = doc.page_content.lower()\n",
    "        found_keywords = sum(1 for kw in keywords if kw.lower() in doc_lower)\n",
    "        if found_keywords > 0:\n",
    "            mrr = 1.0 / (idx + 1)\n",
    "            break\n",
    "    \n",
    "    # Calculate nDCG (simplified version)\n",
    "    dcg = 0.0\n",
    "    idcg = 0.0\n",
    "    for idx in range(min(len(docs), len(keywords))):\n",
    "        doc_lower = docs[idx].page_content.lower()\n",
    "        found_keywords = sum(1 for kw in keywords if kw.lower() in doc_lower)\n",
    "        dcg += found_keywords / (idx + 1)\n",
    "        idcg += 1 / (idx + 1)\n",
    "    \n",
    "    ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "    \n",
    "    return RetrievalResult(\n",
    "        mrr=mrr,\n",
    "        ndcg=ndcg,\n",
    "        keywords_found=keywords_found,\n",
    "        total_keywords=len(keywords),\n",
    "        keyword_coverage=keyword_coverage,\n",
    "        retrieved_docs=[doc.page_content[:200] for doc in docs]\n",
    "    )\n",
    "\n",
    "def evaluate_answer(question, reference_answer, keywords):\n",
    "    \"\"\"\n",
    "    Evaluate generated answer quality\n",
    "    Returns: (AnswerResult, generated_answer, retrieved_docs)\n",
    "    \"\"\"\n",
    "    # Get retrieval\n",
    "    docs = retreiver.invoke(question)\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    # Generate answer\n",
    "    system_prompt = SYSTEM_PROMPT_TEMPLATE.format(context=context)\n",
    "    response = llm1.invoke([\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=question)\n",
    "    ])\n",
    "    generated_answer = response.content\n",
    "    \n",
    "    # Evaluate accuracy (keyword matching)\n",
    "    answer_lower = generated_answer.lower()\n",
    "    keywords_matched = sum(1 for kw in keywords if kw.lower() in answer_lower)\n",
    "    accuracy = (keywords_matched / len(keywords) * 5) if keywords else 5.0\n",
    "    accuracy = min(accuracy, 5.0)\n",
    "    \n",
    "    # Evaluate completeness (length and detail)\n",
    "    ref_words = len(reference_answer.split())\n",
    "    gen_words = len(generated_answer.split())\n",
    "    length_ratio = gen_words / ref_words if ref_words > 0 else 1.0\n",
    "    completeness = 5.0 if 0.7 <= length_ratio <= 1.3 else 3.0\n",
    "    \n",
    "    # Evaluate relevance (semantic similarity)\n",
    "    ref_words_set = set(reference_answer.lower().split())\n",
    "    gen_words_set = set(generated_answer.lower().split())\n",
    "    similarity = len(ref_words_set & gen_words_set) / len(ref_words_set | gen_words_set)\n",
    "    relevance = similarity * 5.0\n",
    "    \n",
    "    # Generate feedback\n",
    "    feedback_parts = []\n",
    "    if keywords_matched < len(keywords):\n",
    "        missing = [kw for kw in keywords if kw.lower() not in answer_lower]\n",
    "        feedback_parts.append(f\"Missing keywords: {', '.join(missing[:3])}\")\n",
    "    if length_ratio < 0.7:\n",
    "        feedback_parts.append(\"Answer too short - lacks detail\")\n",
    "    elif length_ratio > 1.3:\n",
    "        feedback_parts.append(\"Answer too long - could be more concise\")\n",
    "    if similarity < 0.5:\n",
    "        feedback_parts.append(\"Answer diverges from reference - may have hallucinations\")\n",
    "    \n",
    "    feedback = \" | \".join(feedback_parts) if feedback_parts else \"Good answer!\"\n",
    "    \n",
    "    return AnswerResult(\n",
    "        accuracy=accuracy,\n",
    "        completeness=completeness,\n",
    "        relevance=relevance,\n",
    "        feedback=feedback\n",
    "    ), generated_answer, docs\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EVALUATION LOOP\n",
    "# ============================================================================\n",
    "\n",
    "def run_detailed_evaluation(test_cases, num_tests=None):\n",
    "    \"\"\"\n",
    "    Run detailed evaluation on test cases\n",
    "    \"\"\"\n",
    "    if num_tests:\n",
    "        test_cases = test_cases[:num_tests]\n",
    "    \n",
    "    results = {\n",
    "        'individual_tests': [],\n",
    "        'summary': {}\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DETAILED RAG SYSTEM EVALUATION\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Total tests to run: {len(test_cases)}\\n\")\n",
    "    \n",
    "    # Track metrics\n",
    "    all_accuracy = []\n",
    "    all_completeness = []\n",
    "    all_relevance = []\n",
    "    all_mrr = []\n",
    "    all_ndcg = []\n",
    "    all_keyword_coverage = []\n",
    "    \n",
    "    for test_number, test in enumerate(test_cases, 1):\n",
    "        # Print test info\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"Test #{test_number}\")\n",
    "        print(f\"{'=' * 80}\")\n",
    "        print(f\"Question: {test['question']}\")\n",
    "        print(f\"Keywords: {test['keywords']}\")\n",
    "        print(f\"Category: {test['category']}\")\n",
    "        print(f\"Reference Answer: {test['reference_answer'][:150]}...\")\n",
    "        \n",
    "        # Retrieval Evaluation\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(\"Retrieval Evaluation\")\n",
    "        print(f\"{'=' * 80}\")\n",
    "        \n",
    "        retrieval_result = evaluate_retrieval(\n",
    "            test['question'], \n",
    "            test['keywords']\n",
    "        )\n",
    "        \n",
    "        print(f\"MRR: {retrieval_result.mrr:.4f}\")\n",
    "        print(f\"nDCG: {retrieval_result.ndcg:.4f}\")\n",
    "        print(f\"Keywords Found: {retrieval_result.keywords_found}/{retrieval_result.total_keywords}\")\n",
    "        print(f\"Keyword Coverage: {retrieval_result.keyword_coverage:.1f}%\")\n",
    "        print(f\"\\nRetrieved Documents:\")\n",
    "        for i, doc in enumerate(retrieval_result.retrieved_docs, 1):\n",
    "            print(f\"  {i}. {doc}...\")\n",
    "        \n",
    "        # Answer Evaluation\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(\"Answer Evaluation\")\n",
    "        print(f\"{'=' * 80}\")\n",
    "        \n",
    "        answer_result, generated_answer, retrieved_docs = evaluate_answer(\n",
    "            test['question'],\n",
    "            test['reference_answer'],\n",
    "            test['keywords']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nGenerated Answer:\\n{generated_answer}\")\n",
    "        print(f\"\\nFeedback:\\n{answer_result.feedback}\")\n",
    "        print(\"\\nScores:\")\n",
    "        print(f\"  Accuracy: {answer_result.accuracy:.2f}/5\")\n",
    "        print(f\"  Completeness: {answer_result.completeness:.2f}/5\")\n",
    "        print(f\"  Relevance: {answer_result.relevance:.2f}/5\")\n",
    "        print(f\"  Overall: {(answer_result.accuracy + answer_result.completeness + answer_result.relevance)/3:.2f}/5\")\n",
    "        print(f\"{'=' * 80}\\n\")\n",
    "        \n",
    "        # Store results\n",
    "        results['individual_tests'].append({\n",
    "            'test_number': test_number,\n",
    "            'question': test['question'],\n",
    "            'category': test['category'],\n",
    "            'keywords': test['keywords'],\n",
    "            'generated_answer': generated_answer,\n",
    "            'reference_answer': test['reference_answer'],\n",
    "            'retrieval': {\n",
    "                'mrr': retrieval_result.mrr,\n",
    "                'ndcg': retrieval_result.ndcg,\n",
    "                'keyword_coverage': retrieval_result.keyword_coverage,\n",
    "                'keywords_found': retrieval_result.keywords_found\n",
    "            },\n",
    "            'answer': {\n",
    "                'accuracy': answer_result.accuracy,\n",
    "                'completeness': answer_result.completeness,\n",
    "                'relevance': answer_result.relevance,\n",
    "                'feedback': answer_result.feedback\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        # Collect metrics\n",
    "        all_accuracy.append(answer_result.accuracy)\n",
    "        all_completeness.append(answer_result.completeness)\n",
    "        all_relevance.append(answer_result.relevance)\n",
    "        all_mrr.append(retrieval_result.mrr)\n",
    "        all_ndcg.append(retrieval_result.ndcg)\n",
    "        all_keyword_coverage.append(retrieval_result.keyword_coverage)\n",
    "    \n",
    "    # Calculate summary\n",
    "    results['summary'] = {\n",
    "        'total_tests': len(test_cases),\n",
    "        'avg_accuracy': sum(all_accuracy) / len(all_accuracy) if all_accuracy else 0,\n",
    "        'avg_completeness': sum(all_completeness) / len(all_completeness) if all_completeness else 0,\n",
    "        'avg_relevance': sum(all_relevance) / len(all_relevance) if all_relevance else 0,\n",
    "        'avg_mrr': sum(all_mrr) / len(all_mrr) if all_mrr else 0,\n",
    "        'avg_ndcg': sum(all_ndcg) / len(all_ndcg) if all_ndcg else 0,\n",
    "        'avg_keyword_coverage': sum(all_keyword_coverage) / len(all_keyword_coverage) if all_keyword_coverage else 0,\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ============================================================================\n",
    "# PRINT SUMMARY REPORT\n",
    "# ============================================================================\n",
    "\n",
    "def print_summary_report(results):\n",
    "    \"\"\"Print summary statistics\"\"\"\n",
    "    summary = results['summary']\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EVALUATION SUMMARY REPORT\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nTotal Tests Run: {summary['total_tests']}\")\n",
    "    print(f\"\\nAverage Scores:\")\n",
    "    print(f\"  Accuracy:         {summary['avg_accuracy']:.2f}/5.0\")\n",
    "    print(f\"  Completeness:     {summary['avg_completeness']:.2f}/5.0\")\n",
    "    print(f\"  Relevance:        {summary['avg_relevance']:.2f}/5.0\")\n",
    "    print(f\"  Overall:          {(summary['avg_accuracy'] + summary['avg_completeness'] + summary['avg_relevance'])/3:.2f}/5.0\")\n",
    "    \n",
    "    print(f\"\\nRetrieval Metrics:\")\n",
    "    print(f\"  Mean Reciprocal Rank (MRR): {summary['avg_mrr']:.4f}\")\n",
    "    print(f\"  nDCG (Normalized Gains):    {summary['avg_ndcg']:.4f}\")\n",
    "    print(f\"  Keyword Coverage:           {summary['avg_keyword_coverage']:.1f}%\")\n",
    "    print(f\"\\n{'=' * 80}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# USAGE\n",
    "# ============================================================================\n",
    "\n",
    "# Load test data\n",
    "import json\n",
    "test_cases = []\n",
    "with open('tests.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        test_cases.append(json.loads(line.strip()))\n",
    "\n",
    "# Run detailed evaluation (first 5 tests for quick demo)\n",
    "results = run_detailed_evaluation(test_cases, num_tests=None)\n",
    "\n",
    "# Print summary\n",
    "print_summary_report(results)\n",
    "\n",
    "# Save all results\n",
    "with open('detailed_evaluation_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "    print(\"‚úÖ Saved detailed results to: detailed_evaluation_results.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac8acd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f93fb29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6108c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56387a18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f35d150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
